\chapter{Segmentacja obiektów pierwszoplanowych}
\label{cha:tematykaPracy}
Cyfrowa akwizycja obrazu pozwala na odwzorowanie widzialnej dla człowieka sceny w\,postaci zdigitalizowanej. Takie przedstawienie obrazu umożliwia utrwalenie go na rozmaitych nośnikach pamięci i dostęp do niego w dowolnym momencie w przyszłości. Rejestrując zdjęcia z pewną częstotliwością i zachowaniem kolejności ich akwizycji otrzymuje się nagranie wideo. Pozwala ono śledzić zmiany na scenie następujące w czasie, takie jak ruch czy różnice w oświetleniu. Następujące po sobie obrazy zwane \textbf{ramkami} to \textbf{sekwencja wideo}.

\section{Pojęcie segmentacji}
Celem każdej segmentacji jest wyekstrahowanie interesujących fragmentów obrazu. Bardzo często operacja ta prowadzi do uzyskania binarnej, czarno-białej reprezentacji, na której wyodrębnione zostały interesujące fragmenty obrazu wejściowego - na przykład obiekty w\,ruchu bądź na pierwszym planie. Przykładowy wynik takiej operacji zamieszczony jest na rys. \ref{fig:segGt}. Najczęściej segmentacja służy do przygotowania informacji zawartych na zdjęciu do dalszej analizy - jak choćby w celu dokonania pomiarów czy śledzenia obiektów w przypadku sekwencji wideo. Istnieje wiele metod segmentacji, od binaryzacji z wykorzystaniem arbitralnie dobranego progu dla całego obrazu, poprzez rozrost, zalewanie obszaru i wiele innych. Bardzo ważnym jest dobór odpowiedniego sposobu w zależności od danych, z jakimi algorytm będzie miał do czynienia. Zły dobór metody może prowadzić do błędnych, a często nawet i bezużytecznych wyników - binaryzacja z globalnym progiem przy nierównomiernym oświetleniu często nie potrafi sobie poradzić nawet z wyekstrahowaniem tekstu pisanego.

\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{img/segIn}
\caption{\label{fig:segIn}}
\end{subfigure}
\quad
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{img/segGt}
\caption{\label{fig:segGt}}
\end{subfigure}
\quad
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{img/segBad}
\caption{\label{fig:segBad}}
\end{subfigure}
\caption{Przykładowa segmentacja. (a) - ramka wejściowa, (b) - poprawna segmentacja, (c) - błędna segmentacja. \label{fig:segmentacja}}
\end{figure}

\paragraph{}
Segmentacja obiektów pierwszoplanowych staje się szczególnie trudnym zagadnieniem w\,przypadku obecności ruchu na scenie. Wyróżnia się dwa rodzaje ruchu:
\begin{itemize}
\item obiektów pierwszoplanowych - który powinien być wykrywany
\item elementów tła, który powinien być ignorowany
\end{itemize}
Nie można bowiem ograniczyć się wyłącznie do algorytmów badających przemieszczenie elementów sceny, ponieważ w przypadku ich zatrzymania byłyby one błędnie klasyfikowane jako tło. Problematyczne staje się także odseparowywanie informacji uzyskanej z ruchomych fragmentów tła, jak choćby płynącej wody. Przykład niepoprawnej segmentacji w takich warunkach znajduje się na rys. \ref{fig:segBad}.

\section{Obiekty pierwszoplanowe w nagraniach wideo}
Bardzo często systemy wizyjne dokonujące detekcji obiektów pierwszoplanowych mają za zadanie przetworzyć nie jeden obraz, lecz całą sekwencję ramek. Implementacja segmentacji w sposób zbliżony do tego, w jaki odbywa się to u człowieka, jest niezwykle trudna i wymaga dużych nakładów obliczeniowych i pamięciowych - bardzo rozbudowany system nie byłby więc w stanie przeprowadzać obliczeń w czasie rzeczywistym (tj. dokonywać detekcji w czasie krótszym, niż potrzebny na akwizycję kolejnej ramki). Zamiast więc przewidywać, jakie obiekty mogą pojawić się na scenie i wykorzystywać mechanizm uczenia maszynowego, twórcy większości metod poszukują innych rozwiązań.
\paragraph{}
Ze względu na trudność problemu detekcji, opracowanych zostało wiele metod odseparowania obiektów poruszających się od statycznych elementów sceny. Jedną z nich, stosunkowo najprostszą, jest odjęcie od obrazu wejściowego modelu uzyskanego przy pomocy algorytmów generacji tła. Na tym etapie twórcy metod muszą rozważyć jednak trzy podstawowe kwestie:
\begin{itemize}
\item czym jest model i jak się zachowuje?
\item jak dokonać inicjalizacji modelu tła?
\item w jaki sposób aktualizować model w czasie?
\end{itemize}
Związane jest to z faktem, iż bardzo rzadko na sekwencjach wideo rejestrowane są sytuacje idealne - często scenie towarzyszą zmiany oświetlenia (zwłaszcza w długotrwałym monitoringu na zewnątrz) lub szumy/zakłócenia, co skutkuje zmianą wartości poszczególnych pikseli pomimo braku ruchu obiektów. Nieuwzględnienie tego zjawiska może doprowadzić do licznych fałszywych detekcji. Rzadko również do dyspozycji są sekwencje z ''czystym'' obrazem tła - na próbkowanych ramkach znajdują obiekty już w ruchu, albo statyczne, jednak mogące w każdej chwili zmienić swoje położenie - jak choćby zaparkowane samochody. Zasłaniają one tło, przez co wygenerowanie prawidłowego modelu staje się bardzo trudne, a niekiedy niemożliwe. Opisane w dalszej części rozwiązania stworzone zostały z uwzględnieniem niniejszych ograniczeń.
\subsection{Modelowanie tła}
Jednym z najczęściej używanych w segmentacji obiektów pierwszoplanowych rozwiązań pośrednich jest modelowanie tła. Polega ono na stworzeniu modelu referencyjnego ''czystej'' sceny, czyli takiej, na której nie ma żadnych obiektów, co pozwala na późniejsze porównanie z\,nim kolejnych ramek nagrania w celu wykrycia zmian (por. rys. \ref{fig:bgModel}). Najczęściej odbywa się to poprzez algorytmy odejmowania tła - w najprostszym wariancie obliczany jest moduł z różnicy pomiędzy modelem tła a\,aktualnie analizowanym obrazem sceny, co skutkuje otrzymaniem obrazu, na którym niezerowe wartości są interpretowane jako obiekty pierwszoplanowe.
\paragraph{}
W rzeczywistości jednak to zagadnienie okazuje się być o wiele bardziej skomplikowane. W\,nagraniach rejestrowanych przez kamery w monitoringu wizyjnym rzadko dostępny jest materiał, na którym widoczne jest samo tło. Usuwanie wszelkich obiektów ze sceny i tworzenie nowego modelu za każdym razem, kiedy tło zmieni się - na przykład zostanie wybudowany nowy budynek bądź nastąpi przemeblowanie wewnątrz pomieszczenia - jest bardzo uciążliwe. Twórcy oprogramowania zajmującego się automatyczną detekcją obiektów borykają się także z problemami, jakich nastręcza zjawisko ruchu obrotowego Ziemi - zmiennym oświetleniem sceny w zależności od pory dnia. Trzeba bowiem brać pod uwagę, iż na obrazie reprezentowana jest jasność poszczególnych pikseli - im mniej światła, tym ciemniejsze stają się obiekty. Można pomyśleć, iż ta kwestia rozwiązana zostać może poprzez zmianę przestrzeni kolorów i odseparowanie luminancji pikseli od ich barwy poprzez zastosowanie innej przestrzeni barw, jednak i tu pojawiają się problemy - mniej światła oznacza dużo mniej dokładną akwizycję barwy piksela, a co za tym idzie mogą pojawiać się różnice odcieni w czasie. Występuje tu analogia do procesu widzenia człowieka - oko ludzkie potrafi rozpoznawać kształt i ruch nawet przy bardzo słabym oświetleniu, odbywa się to jednak kosztem postrzegania w kolorze. W\,półmroku człowiek rejestruje otoczenie tylko w skali szarości.
\paragraph{}
Z tych i wielu innych powodów zagadnienie modelowania tła staje się kluczowym w przypadku wielu algorytmów. Dobry model referencyjny pozwala bowiem na ograniczenie liczby fałszywych detekcji. Najczęściej proces inicjalizacji modelu odbywa się na podstawie początkowej sekwencji testowej, a następnie wzorzec tła jest uaktualniany poprzez odpowiednie modyfikacje przez cały czas działania programu dokonującego segmentacji. Stosowanych jest wiele podejść, wykorzystujących zarówno proste mechanizmy, jak średnia wartość piksela na obrazie, jak i nieco bardziej zaawansowane, oparte na statystyce i probabilistyce. Warto wspomnieć tu o modelach gaussowskich, pojawiających się w większości proponowanych obecnie rozwiązań. Zasada ich działania opisana zostanie w dalszej części pracy.
\paragraph{}
Standardowe podejścia w dziedzinie generacji tła, jak średnia jasność bądź model Gaussa na podstawie pojedynczej ramki, nie dają jednak spodziewanych rezultatów w przypadkach obecności drobnych zmian na scenie. Obiekty tła pozostające w ciągłym ruchu mogą bowiem przyjmować diametralnie skrajne wartości jasności. Posiadanie więc jednego modelu tła okazuje się nie być wystarczające. Te same algorytmy dla różnych zestawów nagrań wykazują się odmienną skutecznością, co jest zjawiskiem niepożądanym - dobry system detekcji powinien pracować prawidłowo niezależnie od panujących warunków.
\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{img/fg}
\caption{}
\end{subfigure}
\quad
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{img/bg}
\caption{}
\end{subfigure}
\caption{Modelowanie tła. Po lewej ramka z nagrania, po prawej model tła \label{fig:bgModel}}
\end{figure}
\subsection{Metody uaktualniania modelu tła}
Aby utrzymać prawidłową pracę systemu detekcji w zmiennych warunkach, należy dokonywać uaktualniania modelu tła na bieżąco. Polega to na odpowiedniej podmianie wartości niektórych pikseli modelu. Ze względu na politykę stosowaną w tym procesie wyróżniane są dwie metody:
\begin{itemize}
\item konserwatywna
\item ślepa
\end{itemize} 
Metoda \textbf{konserwatywna} (ang. \textit{conservative update}) polega na uaktualnianiu wartości tylko dla tych pikseli, które zaklasyfikowane zostały obecnie jako tło. Choć z pozoru najbardziej intuicyjna, może doprowadzić do poważnych problemów na przykład przy błędnej detekcji elementów tła jako pierwszoplanowych - jeśli nieprawidłowo wykryto obiekt, nie będzie możliwe uaktualnianie modelu dla fałszywie sklasyfikowanych pikseli. Prowadzi to do powstawania tak zwanych \textbf{duchów} (ang. \textit{ghosts}) - efektów błędnej detekcji, które są propagowane w dalszym procesie działaniu algorytmu. Alternatywą dla tego sposobu jest metoda \textbf{ślepa} (ang. \textit{blind update}). Choć zapobiega wspomnianym zakleszczeniom, które mogą zdarzyć się przy polityce konserwatywnej, daje bardzo słabe wyniki dla obiektów poruszających się wolno, jako że piksele należące do obiektów pierwszoplanowych ''przenikają'' do modelu. W rzeczywistych rozwiązaniach stosowane są metody pośrednie, łączące zalety obu polityk.

%\section{Standardowe metody a dynamiczne tło}
\section{Narzędzia}
\subsection{Język C++}
C++ jest jednym z najpopularniejszych obecnie języków programowania. Wywodzi się ze strukturalnego języka C, z tego też powodu utrzymywana jest z nim bardzo wysoka zgodność na poziomie kodu źródłowego. Łączy w sobie kilka paradygmatów programowania: proceduralnego, obiektowego i generycznego. Wybrany został do implementacji rozwiązań ze względu na łatwe korzystanie z bibliotek za jego pośrednictwem i możliwości dostępu do zasobów sprzętowych oraz funkcji systemowych. Jego niewątpliwą zaletą jest wieloplatformowość, powalająca na tworzenie oprogramowania w środowisku wygodnym dla programisty.
\subsection{Biblioteka OpenCV}

OpenCV (ang. \textit{Open Source Computer Vision}) to dostępna od 2000 roku biblioteka, zawierająca implementacje najczęściej wykorzystywanych algorytmów wizyjnych. Jej główne zalety to dostępność na zasadach \textit{open source}, a także wieloplatformowość. Została ona napisana w\,języku C, jednak istnieją specjalne nakładki, pozwalające korzystać z niej także m. in. w C++, C\#, Python i języku Java. Udostępniony publicznie jest nie tylko kod źródłowy, ale także i narzędzia, pozwalające na samodzielną kompilację biblioteki, co pozwala na używanie jej w wielu środowiskach programistycznych.
\paragraph{}
Biblioteka ta zawiera ponad 2500 algorytmów zoptymalizowanych pod względem pamięciowym i obliczeniowym. Jest ciągle rozwijana, co pozwala domniemywać, iż zawarte w niej rozwiązania pozwalają maksymalnie wykorzystać możliwości sprzętowe obecnych urządzeń. Korzystanie z niej zdaje się więc być najbardziej odpowiednie dla problemu opisywanego w\,niniejszej pracy, jako że szybkość przetwarzania wideo jest kluczowa dla detekcji w czasie rzeczywistym.
\paragraph{}
Do rozwiązania problemu rozważanego w tej pracy użyta została biblioteka \textbf{OpenCV 2.4.9}, dostępna od 25.04.2014r na stronie http://opencv.org%\nocite{OpenCV}
.