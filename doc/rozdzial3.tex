\chapter{Analiza aktualnie dostępnych rozwiązań}
\label{cha:analiza}

Choć opisane w poprzednim rozdziale metody dokonują segmentacji w większości poprawnie nawet przy obecności drobnego ruchu na scenie, opracowane zostały nowsze metody, radzące sobie z tym zagadnieniem jeszcze lepiej. W tej części pracy omówionych zostanie kilka z nich, według rankingu changedetection.net dających najlepsze rezultaty dla dynamicznego tła. Zawartych jest w nich wiele różnych podejść do problemu, począwszy od modelowania i subtrakcji tła, poprzez rozwiązania bazujące na heurystyce.

%---------------------------------------------------------------------------

\section{Flux Tensor with Split Gaussian Models}
\label{sec:FTSG}

Jednym z zaproponowanych rozwiązań omawianego w tej pracy problemu jest fuzja dwóch metod detekcji zmian na sekwencji wideo \cite{6910016}. Polega ona na wyliczeniu tensora przepływu (ang. \textit{Flux Tensor}) i modelowaniu tła za pomocą algorytmu bazującego na GMM opisanego w sekcji \ref{sec:GMM} z odseparowanymi modelami tła i pierwszego planu, dostosowującą się automatycznie przestrzennie i czasowo.
Metoda ta dzieli się na trzy główne moduły:
\begin{itemize}
\item detekcja zmian na poziomie piksela - obliczane są osobno modele dla ruchu (\textit{flux tensor} - FT) i dla zmian jasności/kolorów na obrazie (\textit{split Gaussian model} - SG)
\item fuzja otrzymanych wyników - stosując odpowiednie reguły łączone są modele FT i SG, aby zredukować ilość fałszywych detekcji
\item klasyfikacja obiektów - obsługa przypadków obiektów zatrzymanych bądź usuniętych ze sceny
\end{itemize}
\subsection{Flux Tensor}
\label{sec:FT}
Tensor przepływu pozwala na wykrycie zmian przemieszczenia obiektów sceny pomiędzy kolejnymi ramkami nagrania. Może on być definiowany jako wariacja czasowa przepływu optycznego w lokalnej przestrzeni 3D. Wykorzystanie tej metody umożliwia pominięcie kosztownego rozkładu macierzy na wartości i wektory własne przy uzyskiwaniu informacji o ruchu na scenie.\\
Najbardziej istotne dla tej metody są następujące cechy:
\begin{itemize}
\item niewrażliwość na cienie
\item nikła czułość na zmiany oświetlenia na scenie.
\end{itemize} 
W postaci macierzy równanie tensora przedstawia się następująco:
\begin{equation}
\label{eq:FT}
J_{F} =
\begin{bmatrix}
%
\int_\Omega \left\{\frac{\partial^2I}{\partial x\partial t}\right\}^2 \mathrm{d}y   &
%
\int_\Omega \frac{\partial^2I}{\partial x\partial t} \frac{\partial^2I}{\partial y\partial t} \mathrm{d}y  &
%
\int_\Omega \frac{\partial^2I}{\partial x\partial t} \frac{\partial^2I}{\partial t^2} \mathrm{d}y
\\[0.5em]
%
%
\int_\Omega \frac{\partial^2I}{\partial y\partial t} \frac{\partial^2I}{\partial x\partial t} \mathrm{d}y  &
%
\int_\Omega \left\{\frac{\partial^2I}{\partial y\partial t}\right\}^2 \mathrm{d}y   &
%
\int_\Omega \frac{\partial^2I}{\partial y\partial t} \frac{\partial^2I}{\partial t^2} \mathrm{d}y
\\[0.5em]
%
%
\int_\Omega \frac{\partial^2I}{\partial t^2} \frac{\partial^2I}{\partial x\partial t} \mathrm{d}y   &
%
\int_\Omega \frac{\partial^2I}{\partial t^2} \frac{\partial^2I}{\partial y\partial t} \mathrm{d}y   &
%
\int_\Omega \left\{\frac{\partial^2I}{\partial t^2}\right\}^2 \mathrm{d}y
\end{bmatrix}
\end{equation}
Uzyskane w ten sposób elementy macierzy tensora zawierają informacje na temat czasowych zmian gradientu (pola wektorowego, wskazującego kierunki najszybszych wzrostów wartości pola skalarnego, w tym wypadku - obrazu) na scenie. Pozwala to na prostą klasyfikację obiektów jako poruszające się bądź statyczne.\\
Z tego też powodu ślad tensora przepływu, zapisany jako:
\begin{equation}
tr(J_{F}) = 
\int_\Omega ||\frac{\partial}{\partial t}\nabla I||^2\mathrm{d}y
\end{equation}
może zostać bezpośrednio wykorzystany do klasyfikacji pikseli jako należących do obiektu poruszającego się bądź statycznego z pominięciem kosztownej dekompozycji macierzy.
\subsection{Split Gaussian Models}
\label{sec:SG}
Jak wspomniana, modele tła oparte na gaussowskim rozkładzie prawdopodobieństwa są szeroko stosowane w tematyce detekcji ruchu. Również i w metodzie opisywanej w tej sekcji postanowiono skorzystać z podobnego podejścia. Różnica polega jednak na tym, iż zamiast tworzyć jeden multimodalny model łączący tło i pierwszy plan, postanowiono odseparować je, stosując także zmienną ilość gaussianów inkorporowanych do modelu.\\
Każdy nowy piksel porównywany jest z K poprzednimi dystrybucjami, gdzie K to parametr dostosowujący się w zależności od warunków. Zgodność z modelem tła ustalana jest na podstawie wzoru:
\begin{equation}
D_{min}(x,y) = \min_{i\in K}\max_{j\in C}((I_{t}(x,y)-\mu_{i,j})^2-T_{b}\cdot\sigma^2)
\end{equation}
Piksel przypisany zostaje do modelu tła jeśli pasuje do któregokolwiek z gaussianów. $T_{b}$ to ustalony próg, oznaczający liczbę odchyleń standardowych, $\sigma = \sum_{i}^{k}\omega_{i}\sigma_{i}$. Oznacza to, że każdemu pikselowi odpowiadać będzie $K\times C$ modeli Gaussa, gdzie $C$ oznacza ilość kanałów - dla modelu RGB $C = 3$. Uproszczeniem jest stosowanie takiej samej wagi $\omega$ i wariancji $\sigma$ dla wszystkich kanałów.
\paragraph{}
Do modelowania pierwszego planu używany jest pojedynczy model gaussowski. Służy on do rozróżniania obiektów zatrzymanych i niejednoznacznych detekcji, następujących na przykład w obecności szumów. Te drugie występują w obszarach, które zostały zaklasyfikowane jako tło przez FT, natomiast SG wykrył je jako pierwszy plan. Przyporządkowywane są do modelu pierwszego planu tylko jeśli różnica między nimi a ich średnimi wartościami jest mniejsza od pewnego ustalonego progu.
\paragraph{}
Modele tła i pierwszego planu uaktualniane są na bieżąco, aby jak najbardziej ograniczyć ilość fałszywych detekcji spowodowanych zmianami oświetlenia bądź drobnym ruchem sceny. Stosowana jest tu metoda konserwatywna. Do tego procesu stosowane są następujące wzory:
\begin{equation}
\mu_{t} = (1-\alpha)M\mu_{t-1}+\alpha MI_{t}
\end{equation}
\begin{equation}
\sigma_{t}^2 = (1-\alpha)M\sigma_{t-1}^2+\alpha M(I_{t}-\mu)^{T}\alpha(I_{t}-\mu)
\end{equation}
\begin{equation}
\omega_{i,t} = (1-\alpha)\omega_{i,t-1}+\alpha M
\end{equation}
\begin{equation}
M = (1-F_{B})\cup(F_{amb}-F_{S})
\end{equation}
$M$ to maska aktualizacji, $\alpha$ - parametr uczenia się algorytmu, $F_{B}, F_{amb}, F_{S}$ to odpowiednio model pierwszego planu uzyskany za pomocą SG, wykryte rejony niejednoznaczne oraz model statycznych elementów sceny. Model pierwszego planu uaktualniany jest na podstawie negatywu maski dla tła.
\subsection{Fuzja wyników}
\label{sec:fuzja}
Wyniki otrzymywane za pomocą metod \ref{sec:FT} i \ref{sec:SG} uzyskiwane są niezależnie od siebie. W skutek działania algorytmów tworzone są więc dwa modele - jeden, za pomocą tensora przepływu, reprezentujący wyłącznie ruch na scenie, drugi, dzięki SG, pozwalający na wykrycie także obiektów statycznych. Ponieważ jednak FT ma tendencję do tworzenia większych masek obiektów, niż powinny być w rzeczywistości, a metoda SG jest wrażliwa na zmiany oświetlenia i zakłócenia/szumy na scenie, aby uzyskać jak najlepsze rezultaty detekcji, eliminując błędne klasyfikacje pikseli dla każdej z metod, należy połączyć oba te rozwiązania.  Fuzji nie można dokonać jednak na przykład poprzez proste dodanie obu wyników. Zaproponowany został następujący model klasyfikacji:
\begin{algorithm}[H]
 \KwData{model tła BG, maska $F_{F}$, maska $F_{B}$}
 \KwResult{klasyfikacja pikseli jako elementy tła lub pierwszego planu}
 \For{$ x,y \in K$}{
  \uIf {$F_{F}(x,y)$ {\bf and} $F_{B}(x,y)$}{
	I(x,y) należy do ruchomego obiektu pierwszoplanowego}
  \uElseIf{$Ff(x,y)$ {\bf and} $!Fb(x,y)$}{
	I(x,y) należy do efektu halo wokół obiektu}
  \uElseIf{$I(x,y)$ pasuje do modelu pierwszego planu}{
	I(x,y) należy do statycznego obiektu pierwszego planu}
  \uElse{
	I(x,y) to zmiana oświetlenia}
  }
 \caption{Pseudokod mechanizmu fuzji rozwiązań}
\end{algorithm}
\subsection{Wykrywanie obiektów zatrzymanych i usuniętych ze sceny}
Elementem wprowadzającym błędną detekcję jest też problem rozróżnienia obiektów zatrzymanych i usuniętych ze sceny. Choć są to dwa różne przypadki, z punktu widzenia maszyny wyglądają one bardzo podobnie - obszar brany tutaj pod uwagę nie porusza się. W wyniku fuzji opisanej w podrozdziale \ref{sec:fuzja} zarówno piksel obiektu statycznego, jak i odsłoniętego tła, zostają zakwalifikowane jako należące do statycznego pierwszego planu.\\
Zaproponowano odpowiedni mechanizm radzenia sobie z tym problemem, polegający na detekcji brzegów obszaru zainteresowania w aktualnej ramce, modelu tła uzyskanego poprzez subtrakcję tła oraz maski obiektów pierwszoplanowych. Porównanie tak wykrytych brzegów pozwala zaklasyfikować rozważany obszar bądź do tła - przy większym podobieństwie brzegu z aktualnej ramki do brzegu uzyskanego z modelu tła - bądź pierwszego planu w przeciwnym wypadku. 
\section{CwisarDH}
Praca \cite{6910014} opisuje metodę opierającą się na tzw. \textit{pattern matching}, czyli dopasowywaniu do wzorca. Rozwiązanie to wykorzystuje mechanizm sieci neuronowych z pominięciem parametrów wagowych. Głównymi cechami tej metody są:
\begin{itemize}
\item realizowanie obliczeń na podstawie wartości piksela bez konieczności uzyskiwania informacji na temat jego sąsiedztwa
\item prostota przetwarzania materiału wideo
\item użycie mechanizmu sieci neuronowych bez jego modyfikacji
\end{itemize}
Mechanizm wykorzystywanych tu sieci neuronowych oparty jest na węzłach pamięci RAM. Przechowują one informacje o wzorcach występujących w obrazie. Procedura mapowania przebiega w następujący sposób:
\begin{enumerate}
\item Każdemu analizowanemu pikselowi obrazu zostaje przypisany osobny dyskryminator, służący do identyfikowania go w dalszych obliczeniach.
\item Barwy piksela w używanej przestrzeni kolorów reprezentowane są w postaci binarnego obrazu $M \times n$, gdzie $M$ - rozdzielczość barwna, $n$ - ilość kanałów (dla RGB =3); wysokość ''słupka'' oznacza wartość odpowiedniej składowej.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.2\textwidth]{img/sample}
\caption{Ilustracja konwersji}
\end{figure}

\item Każdemu pikselowi z tak uzyskanego obrazu zostaje przypisana etykieta w taki sposób, aby cały obszar został zmapowany oraz każda etykieta identyfikowała $n$ pikseli, których wartości '0' lub '1' łączone są kolejno w $n$-bitowe krotki. W ten sposób obraz zostaje pseudolosowo zmapowany na konkretne adresy RAM.
\item Wartość pod uzyskanym adresem ustawiana jest na '1'.
\end{enumerate}
Procedurę mapowania wykonuje się dla próbek testowych w celu inicjalizacji tła. W ten sposób powstaje pewien wzorzec dla każdego piksela. Detekcja ruchu polega więc na porównaniu z nim każdej nowej wartości - jeśli nie spełniony jest wyznaczony próg podobieństwa, piksel klasyfikowany jest jako należący do obiektu pierwszoplanowego.
\paragraph{}
Po fazie inicjalizacyjnej następuje właściwa detekcja. Każdy z pikseli wejściowych ramki zmapowany zostaje w ten sam sposób, jak podczas sekwencji testowej. Następnie sprawdzane są wartości spod adresów uformowanych przez wszystkie nowo uzyskane krotki i liczona jest ilość wystąpień liczby '1'. Uzyskana liczba to \textit{odpowiedź dyskryminatora} $r$. Całkowite dopasowanie piksela do modelu tła następuje wtedy, gdy odpowiedź jest równa ilości etykiet użytych w mapowaniu $m$. Ponieważ jednak taki przypadek zdarza się rzadko, zwłaszcza jeśli brane jest pod uwagę otoczenie z dynamicznym tłem, w praktyce stosowany jest odpowiedni próg podobieństwa, będący niejako granicą stopnia dopasowania piksela do historii tła. Stosunek $m/r$ wyznacza procent podobieństwa - jeśli jest mniejszy niż przyjęta wartość graniczna, piksel traktowany jest jako pierwszoplanowy.
\subsection{Uaktualnianie modelu tła i obsługa nagłych zmian na scenie}
Metoda przedstawiona w tej sekcji jest również podejściem adaptacyjnym. Stosowana jest tu polityka konserwatywna. Wartość każdego piksela zaklasyfikowanego jako tło używana jest do ponownego treningu dyskryminatora. W ten sposób nowo uzyskane wartości również znajdują się we wzorcu.
\paragraph{}
W celu ulepszenia wydajności algorytmu dla obiektów nagle pojawiających się lub usuniętych ze sceny, dla każdego piksela zaklasyfikowanego jako pierwszoplanowy wykorzystywany jest bufor przechowujący jego wartości. Jeśli zostanie on przepełniony, następuje ponowne trenowanie dyskryminatora (z uprzednim opróżnieniem dotychczasowo przechowywanych danych) na podstawie historii piksela. Bufor jest opróżniany za każdym razem, kiedy piksel zostaje rozpoznany jako tło.
\subsection{Dalsze operacje}
W wyniku klasyfikacji pikseli otrzymywany jest binarny obraz, na którym wyodrębnione zostają obiekty poruszające się. Przed wyświetleniem poddawany jest on operacjom erozji i dylatacji, aby usunąć zakłócenia typu sól i pieprz.
\section{Self-tuning Background Subtraction Algorithm}
\label{sec:BinWang}
Choć w ostatnich latach możliwości obliczeniowe maszyn, zarówno pod względem ilości przetwarzanych danych, jak i prędkości, rozwijały się bardzo dynamicznie, obecni inżynierowie borykają się z problemami nie do przezwyciężenia. Częstotliwość taktowania procesorów o dotychczas stosowanej architekturze nie może być już bardziej zwiększana - spowodowane jest to fizycznymi ograniczeniami sprzętu. Sekwencyjne wykonywanie poleceń uzależnia więc czas działania algorytmu od prędkości obliczeniowej jednostki wykonującej operacje. Dla problemu opisywanego w tej pracy szybkość przetwarzania jest szczególnie ważna - standardowy format wideo to 25 klatek na sekundę. Oznacza to, iż w celu detekcji w czasie rzeczywistym, jedna ramka o określonej rozdzielczości musi zostać przetworzona w czasie 1/25 sekundy. Dlatego też autorzy rozwiązania opisanego w artykule \cite{6910012} postanowili zmierzyć się z tym problemem, opracowując metodę, która wykonuje potrzebne obliczenia dla każdego piksela niezależnie, dokonując klasyfikacji jedynie na podstawie historii jego jasności. Takie podejście umożliwia implementację w architekturze równoległej, jak choćby z wykorzystaniem mocy obliczeniowej GPU (ang. \textit{Graphics processing unit} - procesor graficzny) czy układów FPGA (ang. \textit{Field-programmable gate array}).
\paragraph{}
Głównym założeniem tego rozwiązania jest przetwarzanie na poziomie piksela, co pozwala uniknąć konieczności badania wartości sąsiedztwa, a co za tym idzie - umożliwia prostszą implementację na platformie równoległej. Podobnie jak w przypadku opisywanych wcześniej metod, tworzony model tła składa się ze zbiorów wartości dopuszczalnych dla poszczególnych pikseli, a klasyfikacja odbywa się poprzez sprawdzanie dopasowania do wzorca. Zamiast jednak dokonywać losowej podmiany elementów modelu bądź odrzucać najstarsze wartości, autorzy proponują eliminację próbek na podstawie wyliczanej efektywności danego szablonu.
\subsection{Szablony}
Model tła składa się z $K+1$ szablonów dla każdego piksela: jednego długoterminowego $T_{0}$ i $K$ krótkotrwałych $T_{1}$, $T_{2}$, ..., $T_{K}$. W każdym z nich zawiera się wartość piksela tła $B_{k}$ i miernik efektywności $C_{k}$. Szeregowane są one w taki sposób, by wartość $C$ była największa w $T_{0}$ i malała aż do $T_{K}$. W $T_{0}$ przechowywana jest wartość piksela najczęściej występującego na obrazie.
\subsection{Klasyfikacja}
Segmentacja obiektów pierwszoplanowych odbywa się na podstawie porównania pikseli z aktualnie analizowanej ramki z wartościami tła przechowywanych w szablonach (odpowiednio dla każdego piksela).  Sprawdzana jest odległość wartości obecnego piksela od każdej z próbek tła - jeśli $|B_{k}(x)-I(x)|\leq \varepsilon$ ($\varepsilon$ - próg tolerancji), $x$ sklasyfikowany zostaje on jako tło. Proces ten rozpoczyna się od szablonu $T_{0}$ i przerywany jest w momencie znalezienia dopasowania. Jeśli wszystkie próbki zostaną sprawdzone i okaże się, że piksel nie pasuje do żadnej z nich, traktowany jest jako pierwszy plan.
\paragraph{}
W celu poprawienia wyników detekcja przeprowadzana jest także na obrazie o zmniejszonej rozdzielczości. Pozwala to na eliminację zakłóceń o wysokiej częstotliwości. Aktualna ramka oraz szablony $T_{0}$ i $T_{1}$ próbkowane są w dół za pomocą średnich wartości z bloków $N$x$N$, po czym następuje detekcja analogiczna dla obrazu w wysokiej rozdzielczości. Otrzymane wyniki próbkowane są w górę i następuje łączenie rezultatów - tylko piksele zaklasyfikowane do pierwszego planu w obu procesach utrzymują swą wartość.
\subsection{Uaktualnianie modelu tła}
Każdorazowo po dopasowaniu piksela do modelu tła dla detekcji w wysokiej rozdzielczości przeprowadzana jest inkorporacja nowej wartości. Aktualizacja szablonów przebiega w następujący sposób:
\begin{algorithm}[H]
 \KwData{indeks dopasowania $k$, $x$, $B_{k}(x)$, $I(x)$}
 \KwResult{uaktualniony model tła dla $x$}
 \For{$i \in {0,1,...,K}$}{
  \uIf {$i == k$}{
	$B_{i}(x) = (1-\alpha)B_{i}(x)+\alpha I(x)$ \\
	$C_{i}(x)=C_{i}(x)+1$
	}
  \uElse{
	$C_{i}(x)=C_{i}(x)-1$
  }
  }
  
 \caption{Pseudokod inkorporacji nowej wartości do modelu tła}
\end{algorithm}
 Widać, iż efektywność szablonów zostaje zmieniona - zwiększa się o 1 dla $T_{k}(x)$ (pierwszego, na którego podstawie piksel sklasyfikowany został jako tło), natomiast zmniejsza, również o 1, dla pozostałych. Jeśli wartość $C_{n}(x)$ osiągnie 0, szablon $T_{n}(x)$ eliminowany jest w dalszym procesie detekcji. Używany parametr $\alpha$ odpowiada za szybkość uczenia się nowych wartości. 
 \paragraph{}
Aktualizacja modelu tła powoduje ponowne szeregowanie szablonów względem efektywności. Autorzy rozwiązania proponują w tym celu proste sortowanie bąbelkowe, działające na szablonach od $T_{1}$ do $T_{K}$. Dla długotrwałego $T_{0}$ stosowana jest inna procedura - badana jest efektywność szablonu $T_{1}$; jeśli jest ona większa od pewnego ustalonego progu $\theta_{L}$ oraz od efektywności $T_{0}$, szablony zamieniane są ze sobą, a $C_{0}$ ustawiane jest na $\gamma \times \theta_{L}$.
\paragraph{}
W celu inkorporacji do modelu tła wartości przez odpowiednio długi okres klasyfikowanych jako należące do pierwszego planu, szablony o najmniejszej efektywności są wymieniane na nowe. Służy to poprawie detekcji w przypadku długotrwałych zmian na scenie (np. włączeniu światła) oraz eliminacji błędów powstałych w wyniku błędnej klasyfikacji. Do realizacji tego procesu dla każdego piksela wykorzystywany jest pomocniczy szablon akumulacyjny. Jeśli piksel sklasyfikowany zostanie jako pierwszy plan, sprawdzana jest efektywność szablonu akumulacyjnego $C_{A}(x)$. Jeśli $C_{A}(x)=0$, zwiększona zostaje o 1, a aktualna wartość piksela przypisywana jest do $B_{A}(x)$. W przeciwnym wypadku obliczana jest odległość $dist(B_{A}(x),I(x))$ i jeśli jest ona mniejsza, niż próg $\varepsilon$, $C_{A}$ zwiększone zostaje o 1, jeśli nie - wartość $C_{A}$ maleje, również o 1. Gdy $C_{A}$ osiągnie pewną graniczną wartość, $B_{A}$ brane jest pod uwagę jako nowa wartość tła. Nie jest jednak włączana automatycznie - zamiana wartości w szablonie $T_{K}$ następuje wyłącznie jeśli $B_{A}$ zawiera się w modelu tła któregokolwiek z sąsiadów (brana jest pod uwagę tolerancja odległości $\varepsilon$).
\subsection{Poprawa wyników detekcji}
Z uwagi na możliwość występowania zakłóceń na scenie oraz konieczność dopasowywania parametru $\varepsilon$ w zależności od dynamiki piksela tła, zaprezentowane zostały mechanizmy radzące sobie z takimi zjawiskami. Monitorowana jest aktywność ''migających'' pikseli, czyli takich, które w procesie detekcji w pełnej rozdzielczości wykryte zostały jako pierwszy plan, natomiast w zmniejszonej rozdzielczości przydzielone zostały do modelu tła. Jeśli parametr ich dynamiki $A$, zmieniany każdorazowo przy wykryciu migotania piksela, osiągnie wartość graniczną $\beta_{TH}$, piksel eliminowany jest z wyniku detekcji. Poziom aktywności $A$ wykorzystywany jest także w procesie dynamicznej adaptacji progu $\varepsilon$ - jeśli $A$ przekroczy graniczną wartość $\beta_{INC}$, $\varepsilon = \varepsilon + \delta_{INC}$; w przypadku gdy $A<\beta_{DEC}$, $\varepsilon = \varepsilon - \delta_{DEC}$. \\
\paragraph{}
W metodzie tej stosowana jest reprezentacja YCbCr (\ref{sec:colorSpace}). Wszystkie używane parametry to trójwymiarowe wektory.

\section{SuBSENSE}
Zaletą metody przedstawionej w \cite{stflexible} jest brak konieczności testowania algorytmu dla każdego obserwowanego otoczenia w celu dostosowania parametrów dających najlepsze wyniki. Ze względu na mechanizm sprzężenia zwrotnego zastosowany w tym podejściu, są one dostosowywane automatycznie, pozwalając uzyskać bardzo dobre wyniki nawet w zmieniających się warunkach. SubSENSE (\textit{Self-Balanced SENsitivity SEgmenter}) to kolejna z metod opierających się na subtrakcji tła, postanowiono jednak podejść do tego zagadnienia w nieco inny sposób. Główne operacje realizowane przez ten algorytm to:
\begin{itemize}
\item wykrywanie zmian na poziomie piksela na podstawie informacji o kolorze w poprzednich ramkach oraz przy pomocy metody LBSP (\ref{sec:LBSP})
\item automatyczne dostosowanie parametrów lokalnej czułości
\end{itemize}
Metoda opisywana w tej sekcji pozwala na odmienne traktowanie rejonów, w których detekcja staje się zagadnieniem bardziej skomplikowanym - na przykład dla obszarów drobnego ruchu tła. 
\subsection{LBSP}
\label{sec:LBSP}
W miejsce standardowego rozwiązania polegającego na porównywaniu kolorów pomiędzy kolejnymi ramkami, autorzy rozwiązania zdecydowali się wykorzystać metodę LBSP \cite{bilodeau2013change} (ang. \textit{Local Binary Similarity Patterns}), pozwalającą zastąpić informację o barwie piksela pewnymi deskryptorami, lepiej odzwierciedlającymi jego cechy. Dzięki temu mechanizm subtrakcji tła daje lepsze efekty. Wiele podejść stosowanych w innych rozwiązaniach bazuje na obliczaniu odległości euklidesowej pomiędzy poziomami jasności pikseli na wybranych fragmentach obrazu, jednak takie rozwiązanie często prowadzi do problemów z detekcją ruchu obiektów zajmujących duże obszary na scenie - spowodowane jest to jednorodnym charakterem powierzchni, przez co zmiana nie jest rejestrowana. Zbadano, iż LBSP lepiej radzi sobie z tym problemem. Ponadto, działania na wartościach binarnych są operacjami bardzo szybkimi, co zmniejsza znacznie czas detekcji.
\paragraph{}
Wyznaczanie lokalnego binarnego wzorca podobieństwa odbywa się na dwóch płaszczyznach - wewnątrz rozważanego regionu osobno dla każdej z analizowanych ramek oraz tegoż regionu pomiędzy ramkami. Porównywane są tu wartości piksela centralnego obszaru z pikselami sąsiadującymi. Analizowany fragment R ma rozmiar \textit{n} x \textit{n}, brane pod uwagę piksele sąsiedztwa (nie wszystkie wartości muszą być analizowane, niekiedy porównuje się wybiórczo) oznaczono jako P. Wyznaczanie LBSP odbywa się na podstawie niniejszego wzoru:
\begin{equation}
LBSP_{R}(x_{c},y_{c}) = 
\sum_{p=0}^{P-1}d(i_{p}-i_{c})2^p
\end{equation},
gdzie
\begin{equation}
d(x)=\left\{\substack{
1 \; \mathrm{dla} \; |x|\leq T_{d} \\[0.5em]
0 \; \mathrm{dla} \; |x|>T_{d}}\right.
\end{equation}
, przy czym $T_{d}$ jest progiem podobieństwa, $i_{p}$ - analizowanym pikselem sąsiedztwa $P$, $i_{c}$ - pikselem centralnym obszaru.\\
Centralny piksel może być wybrany zarówno z obszaru, z którego pochodzi analizowane sąsiedztwo, jak i z innego regionu - na tym samym obrazie bądź z innej ramki.
\subsubsection{Subtrakcja tła}
Subtrakcja tła z pomocą LBSP odbywa się na podstawie porównania obecnej ramki z modelem uzyskanym z F pierwszych. 
\paragraph{Faza inicjalizacyjna \\}
Analiza pierwszej ramki opiera się na wyznaczeniu LBSP  jest jedynie wewnątrz obszaru (ip i ic znajdują się na tej samej ramce). Dla regionu 5 x 5 z wybranymi pikselami sąsiedztwa w liczbie 16 otrzymywane są w ten sposób 16-bitowe wyniki. Ponieważ dla każdego kanału (R, G i B) LBSP wyznaczane jest osobno, po połączeniu uzyskuje się ciąg 48-bitowy. Jeśli w sekwencji testowej dany ciąg powtórzony jest przynajmniej B razy, region oznaczony zostaje jako tło. W modelu M zapisane zostają wartości jasności odpowiadających wzorcowi pikseli. Następnie poddane analizie zostają kolejne ramki ze zbioru F, na podstawie których dokonuje się aktualizacji modelu tła. Po fazie inicjalizacyjnej model pozostaje niezmienny.
\paragraph{Detekcja obiektów \\}
Detekcja obiektów odbywa się na zasadzie porównywania kolejnych ramek z modelem tła. LBSP wyznaczane jest tutaj międzyramkowo, to znaczy piksele $i_{p}$ brane są z aktualnej ramki, $i_{c}$ - z odpowiadającego piksela tła. Decyzja o tym, gdzie piksel ma zostać zaklasyfikowany (bg - tło, fg - pierwszy plan), podejmowana jest w następujący sposób:
\begin{equation}
L_{f}(x,y)=\left\{\substack{
bg \quad \text{dla $H(LBSP_{f}(x,y),LBSP_{M}(x,y))\leq T_{h}$} \\[0.5em]
fg \quad \text{dla $H(LBSP_{f}(x,y),LBSP_{M}(x,y))>T_{h}$}}\right.
\end{equation}
, gdzie H(•) - odległość Hamminga (wektorowa miara odmienności dwóch ciągów takiej samej długości), Th - próg odległości.
\subsection{Mechanizm działania}
Początkowy model tła wyznaczany jest na podstawie pierwszych próbek nagrania. Tworzona jest statystyczna mapa wartości każdego piksela. Jeśli aktualna reprezentacja piksela x przetnie się z minimum 2 próbkami, klasyfikowany jest on jako element tła i odpowiadającej mu pozycji na mapie segmentacji przypisana zostaje wartość St(x) = 0. W czasie działania algorytmu, w celu dostosowania się do ewentualnych zmian oświetlenia bądź innych czynników, próbki w modelu tła są losowo zastępowane przez nowe wartości.\\
\begin{LARGE}
\textcolor{red}{DALSZY CIĄG OPISU METODY}
\end{LARGE}
\section{Spectral-360}
\cite{6910013}\\
\begin{LARGE}
\textcolor{red}{OPIS METODY}
\end{LARGE}


